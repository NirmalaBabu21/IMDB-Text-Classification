# -*- coding: utf-8 -*-
"""IMDB - Text Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1J7lQD1sIqK_uGIZlCFktLfBRi-9d2-

# Problem Statement 1

Problem Statement 1: Build a text classification RNN ( Recurrent Neural Network) model using
IMDB dataset.

Import the IMDB dataset using tensorflow_datasets and perform the
following tasks.
Tasks to be performed:

• Import the required libraries

• Shuffle the data for training and create batches of text and label pairs

• Encode the text data

Note: The simplest way to process text for training is using the TextVectorization layer.

• Build a sequential model using tf.keras.Sequential function

• Compile the model

• Train the model using train dataset

• Test the data using tset dataset and evaluate the model by passing a sentence

Note: If the prediction is >= 0.0, it is positive else it is negative
"""


import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
dataset, info = tfds.load("imdb_reviews", with_info = True, as_supervised=True)

info

print(dataset.keys())

train_data,test_data = dataset['train'],dataset['test']

train_data.element_spec

print(info.features)

for example, label in train_data.take(1):
  print("text:", example.numpy())
  print("label:", label.numpy())

BUFFER_SIZE = 10000
BATCH_SIZE = 64
train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_data = test_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

for example, label in train_data.take(1):
  print('texts: ', example.numpy())
  print()
  print('labels: ', label.numpy())
  print(len(label.numpy()))

VOCAB_SIZE = 1000
encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)
encoder.adapt(train_data.map(lambda text, label: text))

vocab = np.array(encoder.get_vocabulary())
vocab[:20]

vocab

len(vocab)

encoded_example = encoder("My name is").numpy()
encoded_example

encoder.get_vocabulary()[398]

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

val_data= train_data.take(100)
train_data= train_data.skip(100)

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history = model.fit(train_data, epochs=10,
                    validation_data=val_data,
                    validation_steps=30)

test_loss, test_acc = model.evaluate(test_data)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

import matplotlib.pyplot as plt


def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')
plt.ylim(0, None)

sample_text = ('The movie was cool. The animation and the graphics '
               'were out of this world. I would recommend this movie.')
predictions = model.predict(np.array([sample_text]))
if predictions >= 0.0:
    print('positive')
else:
    print('negative')

sample_text = ('The movie was not good. I would not recommend this movie. The movie is bad')
predictions = model.predict(np.array([sample_text]))
if predictions >= 0.0:
    print('positive')
else:
    print('negative')